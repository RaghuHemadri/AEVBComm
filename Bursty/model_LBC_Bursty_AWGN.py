# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/182_dCBKb03H4yPhQn29xLmQIL6vxUOgi
"""

# Commented out IPython magic to ensure Python compatibility.
# This script trains the conv1D-based Linear Block Codes/Modulation
# by ZKY 2019/02/15

import os
# %tensorflow_version 1.x 
os.environ['KERAS_BACKEND'] = 'tensorflow'
from keras.utils import to_categorical
from keras.layers import Dense, Dropout, Lambda, BatchNormalization, Input, Conv1D, MaxPool1D
from keras.layers import TimeDistributed, Flatten, Activation, Conv2D, Conv2DTranspose, UpSampling1D
from keras.models import Model
from keras.callbacks import EarlyStopping, TensorBoard, History, ModelCheckpoint, ReduceLROnPlateau
from keras import backend as KR
import numpy as np
import copy
import time
import matplotlib.pyplot as plt
from keras import optimizers 
import keras
from keras import layers

'''
 --- COMMUNICATION PARAMETERS ---
'''

# Bits per Symbol
k = 4

# Number of symbols
L = 50

# Channel Use
n = 2

# Effective Throughput
#  bits per symbol / channel use
R = k / n

# Eb/N0 used for training
train_Eb_dB = 6

# Number of messages used for training, each size = k*L
batch_size = 64
nb_train_word = batch_size*200

# Noise Variance
backendNoise = 1 / (2 * R * 10 ** (train_Eb_dB / 10))

# Probability of burst noise
alpha = 0.05
burst_beta = np.random.binomial(1,alpha,size=(batch_size,L,4*n))


#Set the bursty noise variance
burstyNoise = 1.0

'''
 --- GENERATING INPUT DATA ---
'''

# Generate training binary Data
train_data = np.random.randint(low=0, high=2, size=(nb_train_word, k * L))
# Used as labeled data
label_data = copy.copy(train_data)
train_data = np.reshape(train_data, newshape=(nb_train_word, L, k))

# Convert Binary Data to integer
tmp_array = np.zeros(shape=k)
for i in range(k):
    tmp_array[i] = 2 ** i
int_data = tmp_array[::-1]

# Convert Integer Data to one-hot vector
int_data = np.reshape(int_data, newshape=(k, 1))
one_hot_data = np.dot(train_data, int_data)
vec_one_hot = to_categorical(y=one_hot_data, num_classes=2 ** k)

# used as Label data
label_one_hot = copy.copy(vec_one_hot)

'''
 --- NEURAL NETWORKS PARAMETERS ---
'''

early_stopping_patience = 100

epochs = 150

optimizer = optimizers.RMSprop(lr=0.01, rho=0.9)

early_stopping = EarlyStopping(monitor='val_loss',
                               patience=early_stopping_patience)


# Learning Rate Control
reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1,
                              patience=5, min_lr=0.0001)

# Save the best results based on Training Set
modelcheckpoint = ModelCheckpoint(filepath='./' + 'model_LBC_' + str(k) + '_' + str(L) + '_' + str(n) + '_' + str(train_Eb_dB) + 'dB' + ' ' + 'AWGN' + '.h5',
                                  monitor='val_loss',
                                  verbose=1,
                                  save_best_only=True,
                                  save_weights_only=True,
                                  mode='auto', period=1)

def vae_loss(x, z_decoded):
    #x = KR.flatten(x)
    #z_decoded = KR.flatten(z_decoded)
    # Reconstruction loss
    xent_loss = KR.mean(keras.losses.binary_crossentropy(x, z_decoded))
    # KL divergence
    kl_loss = -0.5 * KR.mean(1 + z_log_var - KR.square(z_mean) - KR.exp(z_log_var))
    return (xent_loss+(1e-4*kl_loss))

# Define Power Norm for Tx
def normalization(x):
    mean = KR.mean(x ** 2)
    return x / KR.sqrt(2 * mean)  # 2 = I and Q channels


# Define Channel Layers including AWGN and Flat Rayleigh fading
#  x: input data
#  sigma: noise std
def sampling(args):
    z_mean, z_log_var = args
    epsilon = KR.random_normal(shape=KR.shape(z_mean))
    return z_mean + KR.exp(0.5 * z_log_var) * epsilon

def channel_layer(x):

    n1 = KR.random_normal(KR.shape(x), mean=0.0, stddev=np.sqrt(burstyNoise))
    n2 = KR.random_normal(KR.shape(x), mean=0.0, stddev=np.sqrt(backendNoise))

    return x + burst_beta*n1 + n2


encoder_input = Input(batch_shape=(batch_size, L, 2 ** k), name='input_bits')

e = Conv1D(filters=256, strides=1, kernel_size=1, name='e_1')(encoder_input)
e = BatchNormalization(name='e_2')(e)
e = Activation('relu', name='e_3')(e)

e = Conv1D(filters=256, strides=1, kernel_size=1, name='e_7')(e)
e = BatchNormalization(name='e_8')(e)
e = Activation('relu', name='e_9')(e)

#e = Conv1D(filters=256, strides=1, kernel_size=1, name='e_10')(e)  # 2 = I and Q channels
#e = BatchNormalization(name='e_11')(e)
#e = Activation('relu', name='e_12')(e)

z_mean = Conv1D(filters=4 * n, strides=1, kernel_size=1, name='mean_1')(e)  # 2 = I and Q channels
#z_mean = BatchNormalization(name='mean_2')(z_mean)
#z_mean = Activation('linear', name='mean_3')(z_mean)

z_log_var = Conv1D(filters=4 * n, strides=1, kernel_size=1, name='var_1')(e)  # 2 = I and Q channels
#z_log_var = BatchNormalization(name='var_2')(z_log_var)
#z_log_var = Activation('softplus', name='var_3')(z_log_var)

z = layers.Lambda(sampling)([z_mean, z_log_var])

z = Lambda(normalization, name='power_norm')(z)

# AWGN channel
y_h = Lambda(channel_layer,name='channel_layer')(z)

# Define Decoder Layers (Receiver)
d = Conv1D(filters=256, strides=1, kernel_size=1, name='d_1')(y_h)
d = BatchNormalization(name='d_2')(d)
d = Activation('relu', name='d_3')(d)

d = Conv1D(filters=256, strides=1, kernel_size=1, name='d_4')(d)
d = BatchNormalization(name='d_5')(d)
d = Activation('relu', name='d_6')(d)

#d = Conv1D(filters=256, strides=1, kernel_size=1, name='d_7')(d)
#d = BatchNormalization(name='d_8')(d)
#d = Activation('relu', name='d_9')(d)

# Output One hot vector and use Softmax to soft decoding
model_output = Conv1D(filters=2 ** k, strides=1, kernel_size=1, name='d_10', activation='softmax')(d)

# Build System Model
#encoder = Model(encoder_input, z)

vae = Model(encoder_input, model_output)
vae.add_loss(vae_loss(encoder_input, model_output))
vae.compile(optimizer='RMSPROP', loss=None)
vae.summary()

print('starting train the NN...')
start = time.clock()

# TRAINING
mod_history = vae.fit(x=vec_one_hot, y=None,
                           batch_size=batch_size,
                            epochs=epochs,
                            verbose=1,
                            validation_split=0.2, callbacks=[modelcheckpoint,reduce_lr])

end = time.clock()

print('The NN has trained ' + str(end - start) + ' s')

# Plot the Training Loss and Validation Loss
hist_dict = mod_history.history

val_loss = hist_dict['val_loss']
loss = hist_dict['loss']
#acc = hist_dict['acc']
# val_acc = hist_dict['val_acc']
print('loss:',loss)
print('val_loss:',val_loss)

epoch = np.arange(1, epochs + 1)

plt.semilogy(epoch,val_loss,label='val_loss')
plt.semilogy(epoch, loss, label='loss')

plt.legend(loc=0)
plt.grid('true')
plt.xlabel('epochs')
plt.ylabel('beta vae loss')

plt.show()
